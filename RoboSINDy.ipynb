{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dab3865",
   "metadata": {},
   "source": [
    "# RoboSINDy: Project Introduction\n",
    "\n",
    "**Authors**: Julian Skifstad and Advaith Balaji\n",
    "\n",
    "This notebook implements the paper \"Data-driven discovery of coordinates and governing equations\" by Champion et al. on the Franka Panda Arm. The goal is to compare this method with e2c. This work aims to explore and evaluate the performance of these approaches in a robotic planar pushing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a706c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ea91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from numpngw import write_apng\n",
    "from IPython.display import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.panda_pushing_env import PandaImageSpacePushingEnv\n",
    "from utils.visualizers import GIFVisualizer, NotebookVisualizer\n",
    "from utils.utils import *\n",
    "from sindy.SINDy import RoboSINDy, SindyDataset, NormalizationTransform, PushingImgSpaceController, img_space_pushing_cost_function\n",
    "from e2c.E2C import RoboE2C, GloboE2C, RoboE2CLoss\n",
    "from utils.process_data_ import process_data\n",
    "from utils.panda_pushing_env import TARGET_POSE_FREE, TARGET_POSE_OBSTACLES, OBSTACLE_HALFDIMS, OBSTACLE_CENTRE, BOX_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4515394",
   "metadata": {},
   "source": [
    "Visualize panda arm environment in gym. Executes 3 random pushes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e082348",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Create the visualizer\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "\n",
    "# Initialize the simulation environment\n",
    "env = PandaImageSpacePushingEnv(visualizer=visualizer,\n",
    "                                render_non_push_motions=True,\n",
    "                                camera_heigh=800,\n",
    "                                camera_width=800,\n",
    "                                grayscale=True,\n",
    "                                done_at_goal=False)\n",
    "env.reset()\n",
    "# Perform a sequence of 3 random actions:\n",
    "states = []\n",
    "for i in tqdm(range(3)):\n",
    "    action_i = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action_i)\n",
    "    states.append(state)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "view_states(states)\n",
    "\n",
    "plt.close(fig)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a83103",
   "metadata": {},
   "source": [
    "Collect a dataset npy file that also included the state_derivatives. This will open the npy file in dataset_path and compute the state discrete time derivatives using central difference formula. It will then repackage states, actions, and state derivatives into a new .npy file which we can use as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fdb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"datasets/collected_data_large_push.npy\"\n",
    "dt = 1/240.0 # time step in pybullet\n",
    "\n",
    "\n",
    "data_npy = np.load(dataset_path, allow_pickle=True)\n",
    "\n",
    "samples = []\n",
    "for item in data_npy:\n",
    "    \n",
    "    states = item['states']\n",
    "    actions = item['actions']\n",
    "    state_derivatives = []\n",
    "    for i in range(1, len(states)-1):\n",
    "        state_derivative = (states[i+1] - states[i-1]) / dt\n",
    "        state_derivatives.append(state_derivative)\n",
    "    state_derivatives = np.array(state_derivatives)\n",
    "\n",
    "    for i, state in enumerate(states[1:-1]):\n",
    "        sample = {\n",
    "            'state': state,\n",
    "            'action': actions[i],\n",
    "            'state_derivative': state_derivatives[i]\n",
    "        }\n",
    "        samples.append(sample)\n",
    "\n",
    "view_states([sample['state'] for sample in samples[:5]])\n",
    "\n",
    "# print info about samples\n",
    "print(f\"Number of samples: {len(samples)}\")\n",
    "print(f\"Sample state shape: {samples[0]['state'].shape}\")\n",
    "print(f\"Sample action shape: {samples[0]['action'].shape}\")\n",
    "print(f\"Sample state derivative shape: {samples[0]['state_derivative'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa089535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some training and datset parameters\n",
    "val_fraction = 0.2\n",
    "batch_size = 64\n",
    "num_epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e1bff",
   "metadata": {},
   "source": [
    "The cell below will construct the Dataset used for RoboSINDy. It reads each sample from the .npy samples we created, normalizes each state and state derivatives, and constructs a dataloader to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843af0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SindyDataset(data=samples)\n",
    "\n",
    "val_size = int(val_fraction * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "tot_train_states = []\n",
    "tot_train_state_derivatives = []\n",
    "for i in range(len(train_loader.dataset)):\n",
    "    s = train_loader.dataset[i]['states']   \n",
    "    sd = train_loader.dataset[i]['state_derivatives']  \n",
    "    tot_train_states.append(s.unsqueeze(0))\n",
    "    tot_train_state_derivatives.append(sd.unsqueeze(0))\n",
    "tot_train_states = torch.cat(tot_train_states,dim=0)  \n",
    "tot_train_state_derivatives = torch.cat(tot_train_state_derivatives, dim=0)  # (N,C,H,W)\n",
    "\n",
    "mean_s = tot_train_states .mean(dim=(0,2,3))\n",
    "std_s = tot_train_states .std( dim=(0,2,3))\n",
    "mean_sd = tot_train_state_derivatives.mean(dim=(0,2,3))\n",
    "std_sd = tot_train_state_derivatives.std( dim=(0,2,3))\n",
    "\n",
    "normalization_constants = {\n",
    "  'mean_state': mean_s,              \n",
    "  'std_state':  std_s,               \n",
    "  'mean_state_derivative': mean_sd,  \n",
    "  'std_state_derivative': std_sd,    \n",
    "}\n",
    "\n",
    "norm_tr = NormalizationTransform(normalization_constants)\n",
    "train_data.dataset.transform = norm_tr\n",
    "val_data.dataset.transform = norm_tr\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "print(f\"Loaded {len(train_loader.dataset)} training samples and {len(val_loader.dataset)} validation samples.\\n\")\n",
    "\n",
    "# get one sample and check quality\n",
    "sample = train_loader.dataset[0]\n",
    "print(f\"Sample state shape: {sample['states'].shape}\")\n",
    "print(f\"Sample action shape: {sample['actions'].shape}\")\n",
    "print(f\"Sample state derivative shape: {sample['state_derivatives'].shape}\")\n",
    "print(f\"Sample state: {sample['states']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de877c3f",
   "metadata": {},
   "source": [
    "# Part 1: SINDy Dynamics model with 2 latent dimensions\n",
    "\n",
    "#### We first train a model that learns the latent dynamics of the planar pushing task with 2 latent variables. The hope is that the 2 latent variables will each capture the dynamics of the object in the $x$ and $y$ direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size)\n",
    "\n",
    "model.train_model(train_loader, num_epochs=num_epochs, learning_rate=0.001)\n",
    "\n",
    "#save the model\n",
    "model_path = 'trained_models/sindy_model_v3_latent2.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e2784",
   "metadata": {},
   "source": [
    "Load model from .pt file. Skip if you just trained the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from state dict\n",
    "\n",
    "### SET THE PATH TO YOUR MODEL HERE ###\n",
    "model_path = 'trained_models/sindy_model_v2_latent2.pt'\n",
    "########################################\n",
    "\n",
    "sindy_dynamics_model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size, latent_dim=2)\n",
    "sindy_dynamics_model.load_state_dict(torch.load(model_path))\n",
    "sindy_dynamics_model.eval()\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "sindy_dynamics_model.xi_coefficients.data = mask_xi_matrix(sindy_dynamics_model.xi_coefficients.data)\n",
    "\n",
    "print(\"Xi coefficients:\")\n",
    "print(sindy_dynamics_model.xi_coefficients.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf3000",
   "metadata": {},
   "source": [
    "Visualize the robot pushing the block using the image space dynamics inferred by SINDy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a892bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "\n",
    "# Control on an obstacle free environment\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "\n",
    "start_state = np.array([0.4, 0.0, 0.0])\n",
    "target_state = np.array([0.7, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.4, 0.3, -np.pi/2])\n",
    "# target_state = np.array([0.4, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.3, 0.3, np.pi])\n",
    "# target_state = np.array([0.0, 0.3, 0.0])\n",
    "                \n",
    "# start_state = np.array([0.6, 0.1, -np.pi/4])\n",
    "# target_state = np.array([0.8, -0.0, 0.0])\n",
    "\n",
    "############### choose number of trials to run ###################\n",
    "num_trials = 3\n",
    "##################################################################\n",
    "\n",
    "for i in range(num_trials):\n",
    "    env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True, target_pose_vis=target_state, start_state=start_state)\n",
    "    env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n",
    "    state_0 = env.reset()\n",
    "    controller = PushingImgSpaceController(env, sindy_dynamics_model, img_space_pushing_cost_function, normalization_constants, num_samples=50, horizon=10)\n",
    "\n",
    "    state = state_0\n",
    "\n",
    "    num_steps_max = 15\n",
    "\n",
    "    trajectory = []\n",
    "    goal_reached = False\n",
    "    for i in range(num_steps_max):\n",
    "        action = controller.control(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # check if we have reached the goal\n",
    "        end_pose = env.get_object_pos_planar()\n",
    "        trajectory.append(end_pose)\n",
    "        goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n",
    "        goal_reached = goal_distance < BOX_SIZE/2\n",
    "        if done or goal_reached:\n",
    "            break\n",
    "\n",
    "    ########### WRITRE PATH TO SAVE TRAJECTORIES ############\n",
    "    trajectory_file = \"trash.txt\"\n",
    "    write_traj_to_file(trajectory=trajectory, filename=trajectory_file)\n",
    "    #########################################################\n",
    "\n",
    "    print(f'GOAL REACHED: {goal_reached}')\n",
    "\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03504cb9",
   "metadata": {},
   "source": [
    "# Part 2: SINDy Dynamics model with 3 latent dimensions\n",
    "\n",
    "#### We will now train a model that learns the latent dynamics of the planar pushing task with **3** latent variables. This model will likely be more expressive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size, latent_dim=3)\n",
    "\n",
    "model.train_model(train_loader, num_epochs=num_epochs, learning_rate=0.001)\n",
    "\n",
    "#save the model\n",
    "model_path = 'trained_models/sindy_model_v9_latent3.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from state dict\n",
    "\n",
    "### SET THE PATH TO YOUR MODEL HERE ###\n",
    "model_path = 'trained_models/sindy_model_v4_latent3.pt'\n",
    "########################################\n",
    "\n",
    "sindy_dynamics_model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size, latent_dim=3)\n",
    "sindy_dynamics_model.load_state_dict(torch.load(model_path))\n",
    "sindy_dynamics_model.eval()\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c297727",
   "metadata": {},
   "outputs": [],
   "source": [
    "sindy_dynamics_model.xi_coefficients.data = mask_xi_matrix(sindy_dynamics_model.xi_coefficients.data)\n",
    "\n",
    "print(\"Xi coefficients:\")\n",
    "print(sindy_dynamics_model.xi_coefficients.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd24f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "\n",
    "# Control on an obstacle free environment\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "\n",
    "start_state = np.array([0.4, 0.0, 0.0])\n",
    "target_state = np.array([0.7, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.4, 0.3, -np.pi/2])\n",
    "# target_state = np.array([0.4, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.3, 0.3, np.pi])\n",
    "# target_state = np.array([0.0, 0.3, 0.0])\n",
    "                \n",
    "# start_state = np.array([0.6, 0.1, -np.pi/4])\n",
    "# target_state = np.array([0.8, -0.0, 0.0])\n",
    "\n",
    "############### choose number of trials to run ###################\n",
    "num_trials = 3\n",
    "##################################################################\n",
    "\n",
    "for i in range(num_trials):\n",
    "    env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True, target_pose_vis=target_state, start_state=start_state)\n",
    "    env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n",
    "    state_0 = env.reset()\n",
    "    controller = PushingImgSpaceController(env, sindy_dynamics_model, img_space_pushing_cost_function, normalization_constants, num_samples=50, horizon=10)\n",
    "\n",
    "    state = state_0\n",
    "\n",
    "    num_steps_max = 15\n",
    "\n",
    "    trajectory = []\n",
    "    goal_reached = False\n",
    "    for i in range(num_steps_max):\n",
    "        action = controller.control(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # check if we have reached the goal\n",
    "        end_pose = env.get_object_pos_planar()\n",
    "        trajectory.append(end_pose)\n",
    "        goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n",
    "        goal_reached = goal_distance < BOX_SIZE/2\n",
    "        if done or goal_reached:\n",
    "            break\n",
    "\n",
    "    ########### WRITRE PATH TO SAVE TRAJECTORIES ############\n",
    "    trajectory_file = \"trash.txt\"\n",
    "    write_traj_to_file(trajectory=trajectory, filename=trajectory_file)\n",
    "    #########################################################\n",
    "\n",
    "    print(f'GOAL REACHED: {goal_reached}')\n",
    "\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa902a3d",
   "metadata": {},
   "source": [
    "# Part 3: SINDy Dynamics model with 1 latent dimension\n",
    "\n",
    "#### We will now train a model that learns the latent dynamics of the planar pushing task with **1** latent variable. I wonder what this will recover?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e93ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL WITH LATENT DIMENSION 1\n",
    "\n",
    "#define the model\n",
    "model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size, latent_dim=1)\n",
    "\n",
    "model.train_model(train_loader, num_epochs=num_epochs, learning_rate=0.001)\n",
    "\n",
    "#save the model\n",
    "model_path = 'trained_models/sindy_model_v12_latent1.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from state dict\n",
    "\n",
    "### SET THE PATH TO YOUR MODEL HERE ###\n",
    "model_path = 'trained_models/sindy_model_v4_latent1.pt'\n",
    "########################################\n",
    "\n",
    "sindy_dynamics_model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size, latent_dim=1)\n",
    "sindy_dynamics_model.load_state_dict(torch.load(model_path))\n",
    "sindy_dynamics_model.eval()\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sindy_dynamics_model.xi_coefficients.data = mask_xi_matrix(sindy_dynamics_model.xi_coefficients.data)\n",
    "\n",
    "print(\"Xi coefficients:\")\n",
    "print(sindy_dynamics_model.xi_coefficients.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "\n",
    "# Control on an obstacle free environment\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "\n",
    "start_state = np.array([0.4, 0.0, 0.0])\n",
    "target_state = np.array([0.7, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.4, 0.3, -np.pi/2])\n",
    "# target_state = np.array([0.4, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.3, 0.3, np.pi])\n",
    "# target_state = np.array([0.0, 0.3, 0.0])\n",
    "                \n",
    "# start_state = np.array([0.6, 0.1, -np.pi/4])\n",
    "# target_state = np.array([0.8, -0.0, 0.0])\n",
    "\n",
    "############### choose number of trials to run ###################\n",
    "num_trials = 3\n",
    "##################################################################\n",
    "\n",
    "for i in range(num_trials):\n",
    "    env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True, target_pose_vis=target_state, start_state=start_state)\n",
    "    env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n",
    "    state_0 = env.reset()\n",
    "    controller = PushingImgSpaceController(env, sindy_dynamics_model, img_space_pushing_cost_function, normalization_constants, num_samples=50, horizon=10)\n",
    "\n",
    "    state = state_0\n",
    "\n",
    "    num_steps_max = 15\n",
    "\n",
    "    trajectory = []\n",
    "    goal_reached = False\n",
    "    for i in range(num_steps_max):\n",
    "        action = controller.control(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # check if we have reached the goal\n",
    "        end_pose = env.get_object_pos_planar()\n",
    "        trajectory.append(end_pose)\n",
    "        goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n",
    "        goal_reached = goal_distance < BOX_SIZE/2\n",
    "        if done or goal_reached:\n",
    "            break\n",
    "\n",
    "    ########### WRITRE PATH TO SAVE TRAJECTORIES ############\n",
    "    trajectory_file = \"trash.txt\"\n",
    "    write_traj_to_file(trajectory=trajectory, filename=trajectory_file)\n",
    "    #########################################################\n",
    "\n",
    "    print(f'GOAL REACHED: {goal_reached}')\n",
    "\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82460cd4",
   "metadata": {},
   "source": [
    "# Part 4: Large latent dimension\n",
    "\n",
    "#### We now train a model with a large function library and large latent dimension (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899067f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL WITH LATENT DIMENSION 8\n",
    "\n",
    "#define the model\n",
    "model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size, latent_dim=8)\n",
    "\n",
    "model.train_model(train_loader, num_epochs=num_epochs, learning_rate=0.001)\n",
    "\n",
    "#save the model\n",
    "model_path = 'trained_models/sindy_model_v1_latent8.pt'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ecb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from state dict\n",
    "\n",
    "### SET THE PATH TO YOUR MODEL HERE ###\n",
    "model_path = 'trained_models/sindy_model_v1_latent8.pt'\n",
    "########################################\n",
    "\n",
    "sindy_dynamics_model = RoboSINDy(input_dim=32*32*1, batch_size=batch_size, latent_dim=8)\n",
    "sindy_dynamics_model.load_state_dict(torch.load(model_path))\n",
    "sindy_dynamics_model.eval()\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sindy_dynamics_model.xi_coefficients.data = mask_xi_matrix(sindy_dynamics_model.xi_coefficients.data)\n",
    "\n",
    "print(\"Xi coefficients:\")\n",
    "print(sindy_dynamics_model.xi_coefficients.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "\n",
    "# Control on an obstacle free environment\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "\n",
    "# start_state = np.array([0.4, 0.0, 0.0])\n",
    "# target_state = np.array([0.7, -0.0, 0.0])\n",
    "\n",
    "start_state = np.array([0.4, 0.3, -np.pi/2])\n",
    "target_state = np.array([0.4, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.3, 0.3, np.pi])\n",
    "# target_state = np.array([0.0, 0.3, 0.0])\n",
    "                \n",
    "# start_state = np.array([0.6, 0.1, -np.pi/4])\n",
    "# target_state = np.array([0.8, -0.0, 0.0])\n",
    "\n",
    "############### choose number of trials to run ###################\n",
    "num_trials = 3\n",
    "##################################################################\n",
    "\n",
    "for i in range(num_trials):\n",
    "    env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True, target_pose_vis=target_state, start_state=start_state)\n",
    "    env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n",
    "    state_0 = env.reset()\n",
    "    controller = PushingImgSpaceController(env, sindy_dynamics_model, img_space_pushing_cost_function, normalization_constants, num_samples=50, horizon=10)\n",
    "\n",
    "    state = state_0\n",
    "\n",
    "    num_steps_max = 15\n",
    "\n",
    "    trajectory = []\n",
    "    goal_reached = False\n",
    "    for i in range(num_steps_max):\n",
    "        action = controller.control(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # check if we have reached the goal\n",
    "        end_pose = env.get_object_pos_planar()\n",
    "        trajectory.append(end_pose)\n",
    "        goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n",
    "        goal_reached = goal_distance < BOX_SIZE/2\n",
    "        if done or goal_reached:\n",
    "            break\n",
    "\n",
    "    ########### WRITRE PATH TO SAVE TRAJECTORIES ############\n",
    "    trajectory_file = \"trash.txt\"\n",
    "    write_traj_to_file(trajectory=trajectory, filename=trajectory_file)\n",
    "    #########################################################\n",
    "\n",
    "    print(f'GOAL REACHED: {goal_reached}')\n",
    "\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a65e66f",
   "metadata": {},
   "source": [
    "# Part 5: Training the Locally Linear Model (2 Dimension Latent Space)\n",
    "\n",
    "#### For comparison, we compare RoboSINDy to another latent dynamics model constrained to be locally linear. This model is based on Embed to Control (E2C) by Watter et. al. (2015). \n",
    "\n",
    "#### Here, we train the model on a 2 dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2c.E2C import RoboE2C, RoboE2CLoss\n",
    "from utils.process_data_ import process_data_multiple_step, process_data\n",
    "\n",
    "# Train the dynamics model\n",
    "LATENT_DIM = 2\n",
    "ACTION_DIM = 3\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "collected_data = np.load('datasets/collected_data_large_push.npy', allow_pickle=True)\n",
    "\n",
    "model = RoboE2C(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n",
    "\n",
    "state_loss_fn = nn.MSELoss()\n",
    "latent_loss_fn = nn.MSELoss()\n",
    "loss = RoboE2CLoss(state_loss_fn, latent_loss_fn, alpha=0.1)\n",
    "\n",
    "# Compute normalization constants\n",
    "train_loader, val_loader, norm_constants = process_data(collected_data, batch_size=batch_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "pbar = tqdm(range(30000))\n",
    "\n",
    "train_losses = []\n",
    "model.train()\n",
    "for epoch_i in pbar:\n",
    "    train_loss_i = 0.\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss(model, states, actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_i += loss.item()\n",
    "\n",
    "    pbar.set_description(f'Latent dim {LATENT_DIM} - Loss: {train_loss_i:.4f}')\n",
    "    train_losses.append(train_loss_i)\n",
    "\n",
    "# ---\n",
    "\n",
    "# plot loss: \n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n",
    "axes = [axes]\n",
    "axes[0].plot(train_losses, label=f'latent_dim: {LATENT_DIM}')\n",
    "axes[0].grid()\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Train Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Train Loss')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# save model:\n",
    "save_filename = 'e2c_derivative_big_data_2dim.pt'\n",
    "torch.save(model.state_dict(), save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "model_path = 'trained_models/e2c_derivative_big_data_2dim.pt'\n",
    "########################################\n",
    "LATENT_DIM = 2\n",
    "ACTION_DIM = 3\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "model = RoboE2C(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n",
    "# model_path = os.path.join('multi_step_latent_dynamics_model.pt')\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "from e2c.E2C import PushingImgSpaceController\n",
    "# Control on an obstacle free environment\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "\n",
    "start_state = np.array([0.4, 0.0, 0.0])\n",
    "target_state = np.array([0.7, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.4, 0.3, -np.pi/2])\n",
    "# target_state = np.array([0.4, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.3, 0.3, np.pi])\n",
    "# target_state = np.array([0.0, 0.3, 0.0])\n",
    "                \n",
    "# start_state = np.array([0.6, 0.1, -np.pi/4])\n",
    "# target_state = np.array([0.8, -0.0, 0.0])\n",
    "\n",
    "############### choose number of trials to run ###################\n",
    "num_trials = 3\n",
    "##################################################################\n",
    "\n",
    "for i in range(num_trials):\n",
    "    env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True, target_pose_vis=target_state, start_state=start_state)\n",
    "    env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n",
    "    state_0 = env.reset()\n",
    "    controller = PushingImgSpaceController(env, model, img_space_pushing_cost_function, norm_constants, num_samples=50, horizon=10)\n",
    "\n",
    "    state = state_0\n",
    "\n",
    "    num_steps_max = 15\n",
    "\n",
    "    trajectory = []\n",
    "    goal_reached = False\n",
    "    for i in range(num_steps_max):\n",
    "        action = controller.control(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # check if we have reached the goal\n",
    "        end_pose = env.get_object_pos_planar()\n",
    "        trajectory.append(end_pose)\n",
    "        goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n",
    "        goal_reached = goal_distance < BOX_SIZE/2\n",
    "        if done or goal_reached:\n",
    "            break\n",
    "\n",
    "    ########### WRITRE PATH TO SAVE TRAJECTORIES ############\n",
    "    trajectory_file = \"trash.txt\"\n",
    "    write_traj_to_file(trajectory=trajectory, filename=trajectory_file)\n",
    "    #########################################################\n",
    "\n",
    "    print(f'GOAL REACHED: {goal_reached}')\n",
    "\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9946d",
   "metadata": {},
   "source": [
    "# Part 6: Training the Locally Linear Model (3 Dimension Latent Space)\n",
    "\n",
    "#### Here, we train the locally linear model on a 3 dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16237cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2c.E2C import RoboE2C, RoboE2CLoss\n",
    "from utils.process_data_ import process_data_multiple_step, process_data\n",
    "\n",
    "# Train the dynamics model\n",
    "LATENT_DIM = 3\n",
    "ACTION_DIM = 3\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "collected_data = np.load('datasets/collected_data_large_push.npy', allow_pickle=True)\n",
    "\n",
    "model = RoboE2C(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n",
    "\n",
    "state_loss_fn = nn.MSELoss()\n",
    "latent_loss_fn = nn.MSELoss()\n",
    "loss_fn = RoboE2CLoss(state_loss_fn, latent_loss_fn, alpha=0.1)\n",
    "\n",
    "# Compute normalization constants\n",
    "train_loader, val_loader, norm_constants = process_data(collected_data, batch_size=batch_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "pbar = tqdm(range(30000))\n",
    "\n",
    "train_losses = []\n",
    "model.train()\n",
    "for epoch_i in pbar:\n",
    "    train_loss_i = 0.\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model, states, actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_i += loss.item()\n",
    "\n",
    "    pbar.set_description(f'Latent dim {LATENT_DIM} - Loss: {train_loss_i:.4f}')\n",
    "    train_losses.append(train_loss_i)\n",
    "\n",
    "# ---\n",
    "\n",
    "# plot loss: \n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n",
    "axes = [axes]\n",
    "axes[0].plot(train_losses, label=f'latent_dim: {LATENT_DIM}')\n",
    "axes[0].grid()\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Train Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Train Loss')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# save model:\n",
    "save_filename = 'robo_e2c_3dim_vn.pt'\n",
    "torch.save(model.state_dict(), save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5712283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "model_path = 'trained_models/robo_e2c_3dim_v2.pt'\n",
    "########################################\n",
    "LATENT_DIM = 3\n",
    "ACTION_DIM = 3\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "model = RoboE2C(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n",
    "# model_path = os.path.join('multi_step_latent_dynamics_model.pt')\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "collected_data = np.load('datasets/collected_data_large_push.npy', allow_pickle=True)\n",
    "train_loader, val_loader, norm_constants = process_data(collected_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "from e2c.E2C import PushingImgSpaceController\n",
    "# Control on an obstacle free environment\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "\n",
    "start_state = np.array([0.4, 0.3, -np.pi/2.0])\n",
    "target_state = np.array([0.4, -0.0, 0.0])\n",
    "\n",
    "############### choose number of trials to run ###################\n",
    "num_trials = 4\n",
    "##################################################################\n",
    "\n",
    "for i in range(num_trials):\n",
    "    env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True, target_pose_vis=target_state, start_state=start_state)\n",
    "    env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n",
    "    state_0 = env.reset()\n",
    "    controller = PushingImgSpaceController(env, model, img_space_pushing_cost_function, norm_constants, num_samples=50, horizon=10)\n",
    "\n",
    "    state = state_0\n",
    "\n",
    "    num_steps_max = 15\n",
    "\n",
    "    trajectory = []\n",
    "    goal_reached = False\n",
    "    for i in range(num_steps_max):\n",
    "        action = controller.control(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # check if we have reached the goal\n",
    "        end_pose = env.get_object_pos_planar()\n",
    "        trajectory.append(end_pose)\n",
    "        goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n",
    "        goal_reached = goal_distance < BOX_SIZE/1.5\n",
    "        if done or goal_reached:\n",
    "            break\n",
    "\n",
    "    ########### WRITRE PATH TO SAVE TRAJECTORIES ############\n",
    "    trajectory_file = \"roboE2C_3dim_horizontal.txt\"\n",
    "    write_traj_to_file(trajectory=trajectory, filename=trajectory_file)\n",
    "    #########################################################\n",
    "\n",
    "    print(f'GOAL REACHED: {goal_reached}')\n",
    "\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340ab15",
   "metadata": {},
   "source": [
    "# Part 7: Training the Globally Linear Model (3 Dimension Latent Space)\n",
    "\n",
    "#### We follow inspiration from the E2C paper and present a globally linear latent space model. We train the globally linear model on a 3 dimensional latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f3262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2c.E2C import GloboE2C, RoboE2CLoss\n",
    "from utils.process_data_ import process_data\n",
    "\n",
    "# Train the dynamics model\n",
    "LATENT_DIM = 3\n",
    "ACTION_DIM = 3\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "collected_data = np.load('datasets/collected_data_large_push.npy', allow_pickle=True)\n",
    "\n",
    "model = GloboE2C(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n",
    "\n",
    "state_loss_fn = nn.MSELoss()\n",
    "latent_loss_fn = nn.MSELoss()\n",
    "loss_fn = RoboE2CLoss(state_loss_fn, latent_loss_fn, alpha=0.1)\n",
    "\n",
    "# Compute normalization constants\n",
    "train_loader, val_loader, norm_constants = process_data(collected_data, batch_size=batch_size)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "pbar = tqdm(range(35000))\n",
    "\n",
    "train_losses = []\n",
    "model.train()\n",
    "for epoch_i in pbar:\n",
    "    train_loss_i = 0.\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model, states, actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_i += loss.item()\n",
    "\n",
    "    pbar.set_description(f'Latent dim {LATENT_DIM} - Loss: {train_loss_i:.4f}')\n",
    "    train_losses.append(train_loss_i)\n",
    "\n",
    "# ---\n",
    "\n",
    "# plot loss: \n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n",
    "axes = [axes]\n",
    "axes[0].plot(train_losses, label=f'latent_dim: {LATENT_DIM}')\n",
    "axes[0].grid()\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Train Loss')\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Train Loss')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# save model:\n",
    "save_filename = 'global_e2c_3dim_v2.pt'\n",
    "torch.save(model.state_dict(), save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from e2c.E2C import GloboE2C, RoboE2CLoss\n",
    "from utils.process_data_ import process_data\n",
    "model_path = 'trained_models/global_e2c_3dim_v2.pt'\n",
    "########################################\n",
    "LATENT_DIM = 3\n",
    "ACTION_DIM = 3\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "model = GloboE2C(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n",
    "# model_path = os.path.join('multi_step_latent_dynamics_model.pt')\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "collected_data = np.load('datasets/collected_data_large_push.npy', allow_pickle=True)\n",
    "train_loader, val_loader, norm_constants = process_data(collected_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06130c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "from e2c.E2C import PushingImgSpaceController\n",
    "# Control on an obstacle free environment\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "hfig = display(fig, display_id=True)\n",
    "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n",
    "start_state = np.array([0.4, 0.3, -np.pi/2.0])\n",
    "target_state = np.array([0.4, -0.0, 0.0])\n",
    "# start_state = np.array([0.4, 0.0, 0.0])\n",
    "# target_state = np.array([0.7, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.4, 0.3, -np.pi/2])\n",
    "# target_state = np.array([0.4, -0.0, 0.0])\n",
    "\n",
    "# start_state = np.array([0.3, 0.3, np.pi])\n",
    "# target_state = np.array([0.0, 0.3, 0.0])\n",
    "                \n",
    "# start_state = np.array([0.6, 0.1, -np.pi/4])\n",
    "# target_state = np.array([0.8, -0.0, 0.0])\n",
    "\n",
    "############### choose number of trials to run ###################\n",
    "num_trials = 1\n",
    "##################################################################\n",
    "\n",
    "for i in range(num_trials):\n",
    "    env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True, target_pose_vis=target_state, start_state=start_state)\n",
    "    env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n",
    "    state_0 = env.reset()\n",
    "    controller = PushingImgSpaceController(env, model, img_space_pushing_cost_function, norm_constants, num_samples=50, horizon=10)\n",
    "\n",
    "    state = state_0\n",
    "\n",
    "    num_steps_max = 15\n",
    "\n",
    "    trajectory = []\n",
    "    goal_reached = False\n",
    "    for i in range(num_steps_max):\n",
    "        action = controller.control(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # check if we have reached the goal\n",
    "        end_pose = env.get_object_pos_planar()\n",
    "        trajectory.append(end_pose)\n",
    "        goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n",
    "        goal_reached = goal_distance < BOX_SIZE/2\n",
    "        if done or goal_reached:\n",
    "            break\n",
    "\n",
    "    ########### WRITRE PATH TO SAVE TRAJECTORIES ############\n",
    "    trajectory_file = \"globoE2C_3dim.txt\"\n",
    "    write_traj_to_file(trajectory=trajectory, filename=trajectory_file)\n",
    "    #########################################################\n",
    "\n",
    "    print(f'GOAL REACHED: {goal_reached}')\n",
    "\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the global coefficients\n",
    "print(model.A)\n",
    "print(model.B)\n",
    "print(model.b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
